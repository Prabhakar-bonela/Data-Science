{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqHBmxZ4hz61F0RwjwUavD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prabhakar-bonela/Data-Science/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ritFNEeW7UYo"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlCMz7QmYUYs",
        "outputId": "49af92f5-44eb-4e6c-fd5c-e2ad972b5b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pyspark\n",
        "# from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.appName(\"AppUsingSparkSession\") .master(\"local[*]\") .getOrCreate()\n",
        "# print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlk0Q1gCj-tz",
        "outputId": "7568cba4-95ea-480c-8175-3b636152aa71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f352a559e70>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Access SparkContext from the SparkSession\n",
        "# sc = spark.sparkContext\n",
        "\n",
        "# # Check the number of available cores\n",
        "# num_cores = sc.defaultParallelism\n",
        "# print(f\"Number of available cores: {num_cores}\")\n",
        "\n",
        "# # Check if any specific configuration for executor cores is set\n",
        "# executor_cores = spark.conf.get(\"spark.executor.cores\", \"Not set\")\n",
        "# print(f\"Number of cores per executor: {executor_cores}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfcSKwzLkT8X",
        "outputId": "8f1563d8-b140-4a9b-efef-542391f4221d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of available cores: 2\n",
            "Number of cores per executor: Not set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating RDD\n",
        "# 3 one by parlleisze from the dataset\n",
        "# 2. by loading from local storage\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"Creating RDD\").getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxxeOupllQD-",
        "outputId": "a3ab07a4-e08c-41c7-a30d-dd7f75daaad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7849954de800>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=spark.sparkContext.getOrCreate()\n",
        "print(sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UoDUx4Hte1p",
        "outputId": "69a0e859-c773-401e-9b29-0d1b278e5d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=Creating RDD>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[1,2,3,4,5,6,7,8,9,10]\n",
        "rdd=sc.parallelize(data,4)# here 4 is the partion we need and by default it is 2\n",
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZVU66QgtpD2",
        "outputId": "2da2354b-fc82-4ee2-8b47-0fa564361b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.getNumPartitions()\n",
        "# gives the partitions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCV32zXluUup",
        "outputId": "2b306439-9562-4ed5-a9a6-021562c2f428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#glom gives the rddd in the partition mode the we cna collect to get the last parition we will get\n",
        "# getting last parition\n",
        "rdd.glom().collect()[3]# gives the\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-VoIxTTvlln",
        "outputId": "af21e6e9-8350-418d-e5bd-117a39714111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature                       \tRDD\t                                    DataFrame\n",
        "# Default Partitions\tDetermined by sc.defaultParallelism.\t        Based on the underlying RDD or input source.\n",
        "# Control Partitions\tUse sc.parallelize(data, numPartitions).\t    Use .repartition(numPartitions) or .coalesce().\n",
        "# Inherit Partitions\tExplicitly specified when creating the RDD.\t  Inherits from RDD if created from one."
      ],
      "metadata": {
        "id": "MLFCM8NwwS8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"hi\").getOrCreate()\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKRMr26fv2ez",
        "outputId": "e1632ec1-0f98-45f9-d539-597885422e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f6854b14610>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=spark.sparkContext.getOrCreate()\n",
        "rdd=sc.parallelize([1,2,3,4,5])\n",
        "l=rdd.collect()\n",
        "# rdd.distinct()\n",
        "# rdd.count()\n",
        "# rdd_map=map(str,rdd)\n",
        "l1=[]\n",
        "for i in l:\n",
        "  print(i*2)\n",
        "  l1.append(i*3)\n",
        "print(l1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOJ8mFf9wKC5",
        "outputId": "8177c5de-7f38-44e5-95e6-f9ce8fb75a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "[3, 6, 9, 12, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = [1, 2, 3, 4, 5]\n",
        "l2 = [10, 20, 30, 40, 50]\n",
        "l1 = list(map(lambda x, y: x * y, l, l2))\n",
        "print(l1)  # Output: [10, 40, 90, 160, 250]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS7g8xcHxbqr",
        "outputId": "8007e417-3655-4f3f-f5df-1b3d17975419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 40, 90, 160, 250]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc=spark.sparkContext.getOrCreate()\n",
        "rdd=sc.parallelize([1,2,3,4,5])\n",
        "rdd_filter=rdd.filter(lambda x:x%2==0)\n",
        "rdd_filter.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4wTA1FyyBhd",
        "outputId": "ba4c169c-5fcc-401b-f818-4731d0e3613c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[28] at collect at <ipython-input-42-1a8165f08ac7>:4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#map keep send a fuction as for each indvidual element(map(str,i))\n",
        "#divides as many partions[[1],[2],[3]]\n",
        "#flapmap makies nested list into single list[1,2,3]\n"
      ],
      "metadata": {
        "id": "aGnuRFid7GsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"WordCountExample\").getOrCreate()\n",
        "\n",
        "# Sample input sentence\n",
        "input_sentence = \"This is a sample sentence with some sample words and words repeat.\"\n",
        "\n",
        "# Create an RDD from the sentence\n",
        "rdd = spark.sparkContext.parallelize([input_sentence])\n",
        "\n",
        "# Step 1: Split sentence into words using flatMap\n",
        "words_rdd = rdd.flatMap(lambda sentence: sentence.split())\n",
        "\n",
        "# Step 2: Create a key-value pair where key is word and value is 1\n",
        "word_pairs = words_rdd.map(lambda word: (word.lower(), 1))\n",
        "\n",
        "# Step 3: Reduce by key to count the occurrences of each word\n",
        "word_count = word_pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Collect the result and print\n",
        "result = word_count.collect()\n",
        "for word, count in result:\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsId7Dsr9CVK",
        "outputId": "025efab5-e0c8-4d02-aca0-4392d1c65d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this: 1\n",
            "is: 1\n",
            "a: 1\n",
            "sample: 2\n",
            "sentence: 1\n",
            "with: 1\n",
            "some: 1\n",
            "words: 2\n",
            "and: 1\n",
            "repeat.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Df"
      ],
      "metadata": {
        "id": "51c41_aA9kz1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "spark=SparkSession .builder.appName(\"DF\").getOrCreate()\n",
        "data=[(1,\"Parbhakar\",True,1.00),(2,\"chinni\",False,10.00)]\n",
        "schema = T.StructType([\n",
        "    T.StructField(\"id\", T.IntegerType(), True),\n",
        "    T.StructField(\"Name\", T.StringType(), True),\n",
        "    T.StructField(\"bool\", T.BooleanType(), True),\n",
        "    T.StructField(\"amount\", T.FloatType(), True)\n",
        "])\n",
        "# schema=T.StructType([T.StructField(\"id\",T.IntegerType,True),\n",
        "#                      T.StructField(\"Name\",T.StringType,True),\n",
        "#                      T.StructField(\"bool\",T.Boolean,True),\n",
        "#                      T.StructField(\"amount\",T.FloatType,True)]\n",
        "#\n",
        "df = spark.createDataFrame(data,schema)\n",
        "#df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n",
        "if not spark.catalog.tableExists(\"df_basic_details\"):  # Check if view exists\n",
        "    df.createOrReplaceTempView(\"df_basic_details\")\n",
        "\n",
        "# Construct the SQL query with corrected column names and semicolons\n",
        "sql_query = \"SELECT Name, id, Amount FROM df_basic_details;\"\n",
        "\n",
        "# Execute the SQL query on the SparkSession and create a new DataFrame `df3`\n",
        "df3 = df.spark.sql(sql_query)\n",
        "\n",
        "# Show the newly created DataFrame `df3` containing the results\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "gc-SJHuw1oCL",
        "outputId": "b5b66407-32a7-46f0-be9d-d66934b880e3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+-----+------+\n",
            "| id|     Name| bool|amount|\n",
            "+---+---------+-----+------+\n",
            "|  1|Parbhakar| true|   1.0|\n",
            "|  2|   chinni|false|  10.0|\n",
            "+---+---------+-----+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'spark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-0cbd93d8ffa9>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Execute the SQL query on the SparkSession and create a new DataFrame `df3`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Show the newly created DataFrame `df3` containing the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'spark'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df1=df.repartition(3)"
      ],
      "metadata": {
        "id": "VUiD6TNe21uq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkJYL7393xRv",
        "outputId": "989bc566-339a-4b18-9872-d75ff92acc5f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # SparkSQL\n",
        "# df.createOrReplaceTempView(\"df_basic_details\")\n",
        "# sql_query=\"select Name ,id,Amount from df_basic_details;\"\n",
        "# df2=df.spark.sql(sql_query)\n",
        "# df2.show()\n",
        "\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Assuming you have the DataFrame `df` from previous code\n",
        "\n",
        "# Create or replace a temporary view named \"df_\n",
        "if not spark.catalog.tableExists(\"df_basic_details\"):  # Check if view exists\n",
        "    df.createOrReplaceTempView(\"df_basic_details\")\n",
        "\n",
        "# Construct the SQL query with corrected column names and semicolons\n",
        "sql_query = \"SELECT Name, id, Amount FROM df_basic_details;\"\n",
        "\n",
        "# Execute the SQL query on the SparkSession and create a new DataFrame `df3`\n",
        "df3 = df.spark.sql(sql_query)\n",
        "\n",
        "# Show the newly created DataFrame `df3` containing the results\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "qzIqe6eP4Lnc",
        "outputId": "ec484863-8843-4ec6-8912-7b5a9a23f741"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'spark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-c7314115dbad>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Execute the SQL query on the SparkSession and create a new DataFrame `df3`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Show the newly created DataFrame `df3` containing the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3127\u001b[0m         \"\"\"\n\u001b[1;32m   3128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3129\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   3130\u001b[0m                 \u001b[0;34m\"'%s' object has no attribute '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m             )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'spark'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for UDF we need do same cretae same Temp and the do the df.spark.register and then use the Sql query"
      ],
      "metadata": {
        "id": "tSZG34SPBeOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}